{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "otgL-OprbLGH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets.voc as VOC\n",
    "\n",
    "\n",
    "# xml library for parsing xml files\n",
    "from xml.etree import ElementTree as et\n",
    "\n",
    "#image transforms\n",
    "#import albumentations as A\n",
    "#from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "#standard libraries\n",
    "#from engine import train_one_epoch, evaluate\n",
    "#import utils\n",
    "#import transforms as T\n",
    "\n",
    "\n",
    "#our dataset file\n",
    "#from dataset import FruitDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see if I have cuda on my desktop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphics card specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 13 06:24:07 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 106...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   38C    P8     6W / 120W |    402MiB /  6075MiB |      5%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1782      G   /usr/lib/xorg/Xorg                 18MiB |\r\n",
      "|    0   N/A  N/A      2129      G   /usr/bin/gnome-shell               69MiB |\r\n",
      "|    0   N/A  N/A      3822      G   /usr/lib/xorg/Xorg                147MiB |\r\n",
      "|    0   N/A  N/A      3983      G   /usr/bin/gnome-shell               30MiB |\r\n",
      "|    0   N/A  N/A      4820      G   ...AAAAAAAA== --shared-files       94MiB |\r\n",
      "|    0   N/A  N/A     29136      G   ...AAAAAAAAA= --shared-files       33MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "xPSfvBoMX_Ck"
   },
   "outputs": [],
   "source": [
    "train_dir = 'data/train_zip/train/'\n",
    "test_dir = 'data/test_zip/test/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jARY65UfX_2k"
   },
   "source": [
    "# Viewing the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our dataset class before creating our train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "class FruitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, files_dir, width, height, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.files_dir = files_dir\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        #sort images for consistency\n",
    "        self.imgs = [image for image in sorted(os.listdir(files_dir)) if image[-3:] == 'jpg']\n",
    "        self.classes = ['_', 'apple', 'banana', 'orange']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.imgs[index]\n",
    "        image_path = os.path.join(self.files_dir, img_name)\n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
    "        \n",
    "        #divide all pixels rgb vals by 255\n",
    "        img_res /= 255.0\n",
    "\n",
    "        #annotation file\n",
    "        annot_filename = img_name[:-4] + '.xml'\n",
    "        annot_file_path = os.path.join(self.files_dir, annot_filename)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        tree = et.parse(annot_file_path)\n",
    "\n",
    "        root = tree.getroot()\n",
    "\n",
    "        wt = img.shape[1]\n",
    "        ht = img.shape[0]\n",
    "\n",
    "        #box coordinates for xml files are extracted\n",
    "        for member in root.findall('object'):\n",
    "            labels.append(self.classes.index(member.find('name').text))\n",
    "            \n",
    "            #bounding box x coords\n",
    "            xmin = int(member.find('bndbox').find('xmin').text)\n",
    "            xmax = int(member.find('bndbox').find('xmax').text)\n",
    "            \n",
    "            #bounding box y coords\n",
    "            ymin = int(member.find('bndbox').find('ymin').text)\n",
    "            ymax = int(member.find('bndbox').find('ymax').text)\n",
    "\n",
    "            xmin_corr = (xmin/wt)*self.width\n",
    "            xmax_corr = (xmax/wt)*self.width\n",
    "            ymin_corr = (ymin/ht)*self.height\n",
    "            ymax_corr = (ymax/ht)*self.height\n",
    "\n",
    "            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
    "\n",
    "        #convert boxes into tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        #areas of the boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        #image_id\n",
    "        image_id = torch.tensor([index])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image=image_res, bboxes=target['boxes'], labels=labels)\n",
    "            img_res = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "\n",
    "        return img_res, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our train dataset and view some files in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here's dataset length:  240\n"
     ]
    }
   ],
   "source": [
    "train_data = FruitDataset(train_dir, 224, 224)\n",
    "print(\"here's dataset length: \", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the image:  (224, 224, 3)\n",
      "target bbox {'boxes': tensor([[ 22.4000,  36.4903, 163.1000,  68.6452],\n",
      "        [ 24.8500,  39.3806, 163.4500,  94.2968],\n",
      "        [ 28.0000,  52.3871, 166.9500, 127.8968],\n",
      "        [ 71.0500,  59.6129, 193.9000, 157.5226]]), 'labels': tensor([2, 2, 2, 2]), 'area': tensor([ 4524.1865,  7611.3750, 10492.0693, 12028.2041]), 'iscrowd': tensor([0, 0, 0, 0]), 'image_id': tensor([78])}\n"
     ]
    }
   ],
   "source": [
    "img, target = train_data[78]\n",
    "print(\"shape of the image: \", img.shape)\n",
    "print(\"target bbox\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "let's see how we can visualize some of our data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "object_detection_with_PyTorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
